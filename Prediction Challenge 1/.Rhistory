}
rf_predict_prob <- function(model, newdata) predict(model, newdata = newdata, type = "prob")[,2]
rf_importance   <- function(model) importance(model)
}
# CART full
cart_parms_full <- if (USE_CART_LOSS) list(split = "gini", loss = matrix(c(0,1,2,0), nrow=2)) else list(split = "gini")
cart_full <- rpart(
as.formula(paste(target, "~", paste(predictor_pool, collapse = " + "))),
data = train0, method = "class", parms = cart_parms_full,
control = rpart.control(cp = cart_cp, minsplit = cart_minsplit, maxdepth = cart_maxdepth)
)
# XGB full — NO EARLY STOPPING (no eval set in full training)
dall   <- xgb.DMatrix(data = X_all, label = as.integer(y_all == levels(y_all)[2]))
xgb_full <- xgb.train(params = xgb_params, data = dall, nrounds = xgb_rounds, verbose = 0)
# --------------------------
# Meta-learner (ridge) on OOF probs
# --------------------------
blend_df <- data.frame(rf = oof_rf, xgb = oof_xgb, cart = oof_cart)
for (j in names(blend_df)) if (anyNA(blend_df[[j]])) blend_df[[j]][is.na(blend_df[[j]])] <- mean(blend_df[[j]], na.rm = TRUE)
meta_x <- as.matrix(blend_df)
meta_y <- as.factor(y_all)
set.seed(SEED)
meta_fit <- cv.glmnet(
x = meta_x,
y = as.numeric(meta_y == levels(meta_y)[2]),
family = "binomial",
alpha = 0,
nfolds = 10,
type.measure = "class"
)
# --------------------------
# Training decision vector
# --------------------------
train_rf_p   <- rf_predict_prob(rf_full, train0[, predictor_pool, drop = FALSE])
train_cart_p <- predict(cart_full, newdata = train0, type = "prob")[,2]
train_xgb_p  <- predict(xgb_full, newdata = xgb.DMatrix(data = X_all))
train_blend_p <- blend_w_rf*train_rf_p + blend_w_xgb*train_xgb_p + blend_w_cart*train_cart_p
train_meta_p  <- drop(predict(meta_fit,
newx = as.matrix(data.frame(rf=train_rf_p, xgb=train_xgb_p, cart=train_cart_p)),
s = "lambda.min", type = "response"))
final_train_p    <- 0.6*train_blend_p + 0.4*train_meta_p
final_train_pred <- ifelse(final_train_p >= final_threshold, levels(y_all)[2], levels(y_all)[1])
train_ID <- if ("ID" %in% names(train0)) train0$ID else seq_len(nrow(train0))
training_decisions <- data.frame(ID = train_ID, decision = final_train_pred)
write.csv(training_decisions, "training_decisions.csv", row.names = FALSE)
cat("Wrote training_decisions.csv\n")
# --------------------------
# Optional: submission on Roy_Test.csv
# --------------------------
if (file.exists(TEST_PATH)) {
test0 <- read.csv(TEST_PATH, header = TRUE, stringsAsFactors = FALSE)
for (c in num_cols) if (c %in% names(test0)) test0[[c]] <- suppressWarnings(as.numeric(test0[[c]]))
for (c in cat_cols) if (c %in% names(test0)) test0[[c]] <- factor(test0[[c]])
if ("startDate" %in% names(test0)) {
td <- suppressWarnings(as.Date(test0$startDate))
if (sum(is.na(td)) < nrow(test0)) test0$startYear <- as.numeric(format(td, "%Y"))
}
test0 <- fe_engineer(test0)
for (c in cat_cols) if (c %in% names(test0)) test0[[c]] <- factor(test0[[c]], levels = fac_levels[[c]])
for (pname in predictor_pool) if (!(pname %in% names(test0))) test0[[pname]] <- NA
test0 <- na_fix(test0)
X_te        <- mm_from_df_aligned(test0)
test_rf_p   <- rf_predict_prob(rf_full, test0[, predictor_pool, drop = FALSE])
test_cart_p <- predict(cart_full, newdata = test0, type = "prob")[,2]
test_xgb_p  <- predict(xgb_full, newdata = xgb.DMatrix(data = X_te))
test_blend_p <- blend_w_rf*test_rf_p + blend_w_xgb*test_xgb_p + blend_w_cart*test_cart_p
test_meta_p  <- drop(predict(meta_fit,
newx = as.matrix(data.frame(rf=test_rf_p, xgb=test_xgb_p, cart=test_cart_p)),
s = "lambda.min", type = "response"))
final_test_p    <- 0.6*test_blend_p + 0.4*test_meta_p
final_test_pred <- ifelse(final_test_p >= final_threshold, levels(y_all)[2], levels(y_all)[1])
submission <- data.frame(ID = if ("ID" %in% names(test0)) test0$ID else seq_len(nrow(test0)),
decision = final_test_pred)
write.csv(submission, "submission.csv", row.names = FALSE)
cat("Wrote submission.csv\n")
} else {
cat("Roy_Test.csv not found. Put it next to this script to write submission.csv\n")
}
# --------------------------
# Quick sanity outputs
# --------------------------
cat("\nClass balance (train):\n"); print(table(train0[[target]]))
if ("lowEstimate" %in% names(train0)) {
cat("\nMean lowEstimate by class:\n")
print(tapply(as.numeric(train0$lowEstimate), train0[[target]], mean, na.rm = TRUE))
}
if ("highEstimate" %in% names(train0)) {
cat("\nMean highEstimate by class:\n")
print(tapply(as.numeric(train0$highEstimate), train0[[target]], mean, na.rm = TRUE))
}
cat("\nTop RF importance (first 10 rows):\n")
print(head(if (USE_RANGER) sort(rf_importance(rf_full), decreasing = TRUE) else rf_importance(rf_full), 10))
# =========================================================
# Car Deals (Dubai) — SPEED++ (GLMNET + Rich OOF Target Encoding)
# - OOF TE for: Make, Model, Location, Make:Model, Make:Location
# - Numeric-only design (compact, fast)
# - Small nonlinear bump: squared ratio terms
# - Alpha sweep {0, 0.25, 0.5, 0.75, 1}; pick best CV λ.min
# - 3-fold CV (change NFOLDS_CV to 5 if you want stabler CV)
# - Writes submission.csv (id, Deal)
# =========================================================
suppressPackageStartupMessages({
library(glmnet)
library(Matrix)
})
# ------------ Config ------------
TRAIN_PATH <- "CarsTrainNew.csv"
TEST_PATH  <- "CarsTestNew+Truncated.csv"
RARE_MIN_COUNT <- 20     # bump a bit for cleaner pairs
TE_SMOOTH_ALPHA <- 3.0   # slightly stronger smoothing
NFOLDS_CV <- 3           # keep fast; try 5 later for stability
# ------------ Load ------------
train <- read.csv(TRAIN_PATH, stringsAsFactors = FALSE)
test  <- read.csv(TEST_PATH,  stringsAsFactors = FALSE)
# ------------ Basic types ------------
y <- factor(train$Deal, levels = c("Average","Bad","Good"))
num_cols <- c("Price","Mileage","ValueBenchmark")
for (c in num_cols) {
train[[c]] <- suppressWarnings(as.numeric(train[[c]]))
test[[c]]  <- suppressWarnings(as.numeric(test[[c]]))
}
cat_cols <- c("Make","Model","Location")
for (c in cat_cols) {
train[[c]] <- as.factor(train[[c]])
test[[c]]  <- as.factor(test[[c]])
}
# Lump rare levels (helps TE & pairs)
lump_rare <- function(f, min_count) {
tab <- table(f)
keep <- names(tab)[tab >= min_count]
g <- as.character(f)
g[!(g %in% keep)] <- "OTHER"
factor(g, levels = c(sort(keep), "OTHER"))
}
train$Make  <- lump_rare(train$Make,  RARE_MIN_COUNT)
train$Model <- lump_rare(train$Model, RARE_MIN_COUNT)
# align test to train (and guarantee "OTHER" level)
test$Make  <- factor(as.character(test$Make),  levels = union(levels(train$Make),  "OTHER"))
test$Model <- factor(as.character(test$Model), levels = union(levels(train$Model), "OTHER"))
test$Make[is.na(test$Make)]   <- "OTHER"
test$Model[is.na(test$Model)] <- "OTHER"
# align Location levels (include OTHER for safety)
train$Location <- factor(train$Location, levels = union(levels(train$Location), "OTHER"))
test$Location  <- factor(as.character(test$Location), levels = union(levels(train$Location), "OTHER"))
test$Location[is.na(test$Location)] <- "OTHER"
# ------------ Pair features (very predictive) ------------
train$MakeModel <- interaction(train$Make, train$Model, drop = TRUE)
test$MakeModel  <- interaction(test$Make,  test$Model,  drop = TRUE)
train$MakeLoc   <- interaction(train$Make, train$Location, drop = TRUE)
test$MakeLoc    <- interaction(test$Make,  test$Location,  drop = TRUE)
# Lump rare pairs to keep size sane
train$MakeModel <- lump_rare(train$MakeModel, RARE_MIN_COUNT)
test$MakeModel  <- factor(as.character(test$MakeModel), levels = union(levels(train$MakeModel), "OTHER"))
test$MakeModel[is.na(test$MakeModel)] <- "OTHER"
train$MakeLoc <- lump_rare(train$MakeLoc, RARE_MIN_COUNT)
test$MakeLoc  <- factor(as.character(test$MakeLoc), levels = union(levels(train$MakeLoc), "OTHER"))
test$MakeLoc[is.na(test$MakeLoc)] <- "OTHER"
# ------------ Numeric features (+ tiny nonlinear bump) ------------
safe_log1p <- function(x) log(pmax(x, 0) + 1)
train$logPrice          <- safe_log1p(train$Price)
train$logMileage        <- safe_log1p(train$Mileage)
train$logValueBenchmark <- safe_log1p(train$ValueBenchmark)
test$logPrice           <- safe_log1p(test$Price)
test$logMileage         <- safe_log1p(test$Mileage)
test$logValueBenchmark  <- safe_log1p(test$ValueBenchmark)
eps <- 1e-6
train$Price_to_Benchmark <- train$Price / (train$ValueBenchmark + eps)
test$Price_to_Benchmark  <-  test$Price / ( test$ValueBenchmark + eps)
train$price_minus_bench  <- train$Price - train$ValueBenchmark
test$price_minus_bench   <-  test$Price -  test$ValueBenchmark
train$log_ratio <- log(train$Price + 1) - log(train$ValueBenchmark + 1)
test$log_ratio  <- log( test$Price + 1) - log( test$ValueBenchmark + 1)
# small nonlinearities (still cheap)
train$Price_to_Benchmark_sq <- train$Price_to_Benchmark^2
test$Price_to_Benchmark_sq  <- test$Price_to_Benchmark^2
train$log_ratio_sq <- train$log_ratio^2
test$log_ratio_sq  <- test$log_ratio^2
# ------------ Leakage-safe OOF Target Encoding ------------
te_oof <- function(f_train, f_test, y, nfolds, alpha, prefix) {
set.seed(42)
f_train <- droplevels(f_train)
# ensure "OTHER" present in test levels to avoid warnings
f_test  <- factor(as.character(f_test), levels = union(levels(f_train), "OTHER"))
f_test[is.na(f_test)] <- "OTHER"
n <- length(y)
folds <- sample(rep(1:nfolds, length.out = n))
classes <- levels(y); K <- length(classes)
col_names <- paste0(prefix, "_", classes)
prior <- prop.table(table(y)); prior <- prior[classes]
TE_train <- matrix(NA_real_, nrow = n, ncol = K)
colnames(TE_train) <- col_names
full_tab <- table(f_train, y); full_tab <- full_tab[, classes, drop = FALSE]
lvl_counts <- rowSums(full_tab)
full_TE <- sapply(classes, function(cl) {
num <- full_tab[, cl] + alpha * prior[cl]
den <- lvl_counts + alpha
as.numeric(num / den)
})
colnames(full_TE) <- col_names
full_df <- as.data.frame(full_TE)
full_df$level <- rownames(full_df); rownames(full_df) <- full_df$level; full_df$level <- NULL
for (k in 1:nfolds) {
tr_idx <- which(folds != k); va_idx <- which(folds == k)
f_tr <- f_train[tr_idx]; y_tr <- y[tr_idx]; f_va <- f_train[va_idx]
tab <- table(f_tr, y_tr); tab <- tab[, classes, drop = FALSE]
lvl_cnt <- rowSums(tab)
enc_k <- sapply(classes, function(cl) {
num <- tab[, cl] + alpha * prior[cl]
den <- lvl_cnt + alpha
as.numeric(num / den)
})
colnames(enc_k) <- col_names
enc_df <- as.data.frame(enc_k)
enc_df$level <- rownames(enc_df); rownames(enc_df) <- enc_df$level; enc_df$level <- NULL
TE_va <- matrix(rep(as.numeric(prior), each = length(va_idx)),
nrow = length(va_idx), ncol = K, byrow = FALSE)
colnames(TE_va) <- col_names
lvls <- as.character(f_va); seen <- lvls %in% rownames(enc_df)
if (any(seen)) TE_va[seen, ] <- as.matrix(enc_df[lvls[seen], , drop = FALSE])
TE_train[va_idx, ] <- TE_va
}
TE_test <- matrix(rep(as.numeric(prior), each = length(f_test)),
nrow = length(f_test), ncol = K, byrow = FALSE)
colnames(TE_test) <- col_names
lvls_t <- as.character(f_test); seen_t <- lvls_t %in% rownames(full_df)
if (any(seen_t)) TE_test[seen_t, ] <- as.matrix(full_df[lvls_t[seen_t], , drop = FALSE])
list(train = TE_train, test = TE_test)
}
# Build TE for base and pair features
te_make      <- te_oof(train$Make,      test$Make,      y, NFOLDS_CV, TE_SMOOTH_ALPHA, "Make")
te_model     <- te_oof(train$Model,     test$Model,     y, NFOLDS_CV, TE_SMOOTH_ALPHA, "Model")
te_loc       <- te_oof(train$Location,  test$Location,  y, NFOLDS_CV, TE_SMOOTH_ALPHA, "Loc")
te_makemodel <- te_oof(train$MakeModel, test$MakeModel, y, NFOLDS_CV, TE_SMOOTH_ALPHA, "MakeModel")
te_makeloc   <- te_oof(train$MakeLoc,   test$MakeLoc,   y, NFOLDS_CV, TE_SMOOTH_ALPHA, "MakeLoc")
# ------------ Assemble numeric-only matrices ------------
num_feats <- c(
"Price","Mileage","ValueBenchmark",
"logPrice","logMileage","logValueBenchmark",
"Price_to_Benchmark","price_minus_bench","log_ratio",
"Price_to_Benchmark_sq","log_ratio_sq"
)
TrainNum <- as.matrix(train[, num_feats])
TestNum  <- as.matrix(test[,  num_feats])
X  <- cbind(TrainNum,
te_make$train, te_model$train, te_loc$train,
te_makemodel$train, te_makeloc$train)
Xt <- cbind(TestNum,
te_make$test, te_model$test, te_loc$test,
te_makemodel$test, te_makeloc$test)
X  <- as(Matrix(X,  sparse = TRUE), "dgCMatrix")
Xt <- as(Matrix(Xt, sparse = TRUE), "dgCMatrix")
keep <- complete.cases(as.matrix(X), y)
X <- X[keep, ]; y <- y[keep]
# ------------ cv.glmnet — wider alpha sweep (still tiny/fast) ------------
alphas <- c(0.00, 0.25, 0.50, 0.75, 1.00)
run_cv <- function(alpha_val) {
set.seed(42)
cvfit <- cv.glmnet(
x = X, y = y,
family = "multinomial",
type.measure = "class",
nfolds = NFOLDS_CV,
alpha = alpha_val,
standardize = TRUE
)
lam <- cvfit$lambda.min
acc <- 1 - cvfit$cvm[ which(cvfit$lambda == lam) ]
list(cvfit = cvfit, lambda = lam, acc = as.numeric(acc), alpha = alpha_val)
}
results <- lapply(alphas, run_cv)
best <- results[[ which.max(vapply(results, function(r) r$acc, numeric(1))) ]]
cat(sprintf("\nSelected alpha = %.2f | %d-fold CV Accuracy (lambda.min) ≈ %.4f\n",
best$alpha, NFOLDS_CV, best$acc))
# ------------ Predict & write ------------
pred_test <- predict(best$cvfit, newx = Xt, s = best$lambda, type = "class")
pred_test <- factor(pred_test, levels = levels(y))
submission <- if (!"id" %in% names(test)) {
data.frame(id = seq_len(nrow(test)), Deal = pred_test)
} else {
data.frame(id = test$id, Deal = pred_test)
}
write.csv(submission, "submission.csv", row.names = FALSE)
cat("\nWrote submission.csv (id, Deal)\n")
# ------------ Quick sanity ------------
cat("\nTrain class distribution:\n"); print(prop.table(table(y)))
cat("\nTest prediction distribution:\n"); print(prop.table(table(submission$Deal)))
# =========================================================
# Car Deals (Dubai) — SPEED++ (GLMNET + Rich OOF Target Encoding)
# - OOF TE for: Make, Model, Location, Make:Model, Make:Location
# - Numeric-only design (compact, fast)
# - Small nonlinear bump: squared ratio terms
# - Alpha sweep {0, 0.25, 0.5, 0.75, 1}; pick best CV λ.min
# - 5-fold CV for more stable estimate
# - Writes submission.csv (id, Deal)
# =========================================================
suppressPackageStartupMessages({
library(glmnet)
library(Matrix)
})
set.seed(42)
# ------------ Config ------------
TRAIN_PATH <- "CarsTrainNew.csv"
TEST_PATH  <- "CarsTestNew+Truncated.csv"
RARE_MIN_COUNT    <- 20    # rare-level lumping threshold
TE_SMOOTH_ALPHA   <- 5.0   # stronger smoothing for target encoding
NFOLDS_CV         <- 5     # more stable than 3-fold
# ------------ Load ------------
train <- read.csv(TRAIN_PATH, stringsAsFactors = FALSE)
test  <- read.csv(TEST_PATH,  stringsAsFactors = FALSE)
# ------------ Basic types ------------
y <- factor(train$Deal, levels = c("Average","Bad","Good"))
num_cols <- c("Price","Mileage","ValueBenchmark")
for (c in num_cols) {
train[[c]] <- suppressWarnings(as.numeric(train[[c]]))
test[[c]]  <- suppressWarnings(as.numeric(test[[c]]))
}
cat_cols <- c("Make","Model","Location")
for (c in cat_cols) {
train[[c]] <- as.factor(train[[c]])
test[[c]]  <- as.factor(test[[c]])
}
# Lump rare levels (helps TE & pairs)
lump_rare <- function(f, min_count) {
tab  <- table(f)
keep <- names(tab)[tab >= min_count]
g <- as.character(f)
g[!(g %in% keep)] <- "OTHER"
factor(g, levels = c(sort(keep), "OTHER"))
}
train$Make  <- lump_rare(train$Make,  RARE_MIN_COUNT)
train$Model <- lump_rare(train$Model, RARE_MIN_COUNT)
# align test to train (and guarantee "OTHER" level)
test$Make  <- factor(as.character(test$Make),
levels = union(levels(train$Make), "OTHER"))
test$Model <- factor(as.character(test$Model),
levels = union(levels(train$Model), "OTHER"))
test$Make[is.na(test$Make)]   <- "OTHER"
test$Model[is.na(test$Model)] <- "OTHER"
# align Location levels (include OTHER for safety)
train$Location <- factor(train$Location,
levels = union(levels(train$Location), "OTHER"))
test$Location  <- factor(as.character(test$Location),
levels = union(levels(train$Location), "OTHER"))
test$Location[is.na(test$Location)] <- "OTHER"
# ------------ Pair features (very predictive) ------------
train$MakeModel <- interaction(train$Make, train$Model, drop = TRUE)
test$MakeModel  <- interaction(test$Make,  test$Model,  drop = TRUE)
train$MakeLoc   <- interaction(train$Make, train$Location, drop = TRUE)
test$MakeLoc    <- interaction(test$Make,  test$Location,  drop = TRUE)
# Lump rare pairs to keep size sane
train$MakeModel <- lump_rare(train$MakeModel, RARE_MIN_COUNT)
test$MakeModel  <- factor(as.character(test$MakeModel),
levels = union(levels(train$MakeModel), "OTHER"))
test$MakeModel[is.na(test$MakeModel)] <- "OTHER"
train$MakeLoc <- lump_rare(train$MakeLoc, RARE_MIN_COUNT)
test$MakeLoc  <- factor(as.character(test$MakeLoc),
levels = union(levels(train$MakeLoc), "OTHER"))
test$MakeLoc[is.na(test$MakeLoc)] <- "OTHER"
# ------------ Numeric features (+ tiny nonlinear bump) ------------
safe_log1p <- function(x) log(pmax(x, 0) + 1)
train$logPrice          <- safe_log1p(train$Price)
train$logMileage        <- safe_log1p(train$Mileage)
train$logValueBenchmark <- safe_log1p(train$ValueBenchmark)
test$logPrice           <- safe_log1p(test$Price)
test$logMileage         <- safe_log1p(test$Mileage)
test$logValueBenchmark  <- safe_log1p(test$ValueBenchmark)
eps <- 1e-6
train$Price_to_Benchmark <- train$Price / (train$ValueBenchmark + eps)
test$Price_to_Benchmark  <-  test$Price / ( test$ValueBenchmark + eps)
train$price_minus_bench  <- train$Price - train$ValueBenchmark
test$price_minus_bench   <-  test$Price -  test$ValueBenchmark
train$log_ratio <- log(train$Price + 1) - log(train$ValueBenchmark + 1)
test$log_ratio  <- log( test$Price + 1) - log( test$ValueBenchmark + 1)
# small nonlinearities (still cheap)
train$Price_to_Benchmark_sq <- train$Price_to_Benchmark^2
test$Price_to_Benchmark_sq  <- test$Price_to_Benchmark^2
train$log_ratio_sq <- train$log_ratio^2
test$log_ratio_sq  <- test$log_ratio^2
# ------------ Leakage-safe OOF Target Encoding ------------
te_oof <- function(f_train, f_test, y, nfolds, alpha, prefix) {
set.seed(42)
f_train <- droplevels(f_train)
# ensure "OTHER" present in test levels to avoid warnings
f_test  <- factor(as.character(f_test), levels = union(levels(f_train), "OTHER"))
f_test[is.na(f_test)] <- "OTHER"
n <- length(y)
folds <- sample(rep(1:nfolds, length.out = n))
classes <- levels(y); K <- length(classes)
col_names <- paste0(prefix, "_", classes)
prior <- prop.table(table(y)); prior <- prior[classes]
TE_train <- matrix(NA_real_, nrow = n, ncol = K)
colnames(TE_train) <- col_names
full_tab   <- table(f_train, y)
full_tab   <- full_tab[, classes, drop = FALSE]
lvl_counts <- rowSums(full_tab)
full_TE <- sapply(classes, function(cl) {
num <- full_tab[, cl] + alpha * prior[cl]
den <- lvl_counts + alpha
as.numeric(num / den)
})
colnames(full_TE) <- col_names
full_df <- as.data.frame(full_TE)
full_df$level <- rownames(full_df)
rownames(full_df) <- full_df$level
full_df$level <- NULL
for (k in 1:nfolds) {
tr_idx <- which(folds != k)
va_idx <- which(folds == k)
f_tr <- f_train[tr_idx]
y_tr <- y[tr_idx]
f_va <- f_train[va_idx]
tab    <- table(f_tr, y_tr)
tab    <- tab[, classes, drop = FALSE]
lvl_cnt <- rowSums(tab)
enc_k <- sapply(classes, function(cl) {
num <- tab[, cl] + alpha * prior[cl]
den <- lvl_cnt + alpha
as.numeric(num / den)
})
colnames(enc_k) <- col_names
enc_df <- as.data.frame(enc_k)
enc_df$level <- rownames(enc_df)
rownames(enc_df) <- enc_df$level
enc_df$level <- NULL
TE_va <- matrix(rep(as.numeric(prior), each = length(va_idx)),
nrow = length(va_idx), ncol = K, byrow = FALSE)
colnames(TE_va) <- col_names
lvls <- as.character(f_va)
seen <- lvls %in% rownames(enc_df)
if (any(seen)) {
TE_va[seen, ] <- as.matrix(enc_df[lvls[seen], , drop = FALSE])
}
TE_train[va_idx, ] <- TE_va
}
TE_test <- matrix(rep(as.numeric(prior), each = length(f_test)),
nrow = length(f_test), ncol = K, byrow = FALSE)
colnames(TE_test) <- col_names
lvls_t <- as.character(f_test)
seen_t <- lvls_t %in% rownames(full_df)
if (any(seen_t)) {
TE_test[seen_t, ] <- as.matrix(full_df[lvls_t[seen_t], , drop = FALSE])
}
list(train = TE_train, test = TE_test)
}
# Build TE for base and pair features
te_make      <- te_oof(train$Make,      test$Make,      y, NFOLDS_CV, TE_SMOOTH_ALPHA, "Make")
te_model     <- te_oof(train$Model,     test$Model,     y, NFOLDS_CV, TE_SMOOTH_ALPHA, "Model")
te_loc       <- te_oof(train$Location,  test$Location,  y, NFOLDS_CV, TE_SMOOTH_ALPHA, "Loc")
te_makemodel <- te_oof(train$MakeModel, test$MakeModel, y, NFOLDS_CV, TE_SMOOTH_ALPHA, "MakeModel")
te_makeloc   <- te_oof(train$MakeLoc,   test$MakeLoc,   y, NFOLDS_CV, TE_SMOOTH_ALPHA, "MakeLoc")
# ------------ Assemble numeric-only matrices ------------
num_feats <- c(
"Price","Mileage","ValueBenchmark",
"logPrice","logMileage","logValueBenchmark",
"Price_to_Benchmark","price_minus_bench","log_ratio",
"Price_to_Benchmark_sq","log_ratio_sq"
)
TrainNum <- as.matrix(train[, num_feats])
TestNum  <- as.matrix(test[,  num_feats])
X  <- cbind(
TrainNum,
te_make$train, te_model$train, te_loc$train,
te_makemodel$train, te_makeloc$train
)
Xt <- cbind(
TestNum,
te_make$test, te_model$test, te_loc$test,
te_makemodel$test, te_makeloc$test
)
X  <- as(Matrix(X,  sparse = TRUE), "dgCMatrix")
Xt <- as(Matrix(Xt, sparse = TRUE), "dgCMatrix")
keep <- complete.cases(as.matrix(X), y)
X <- X[keep, ]
y <- y[keep]
# ------------ cv.glmnet — wider alpha sweep ------------
alphas <- c(0.00, 0.25, 0.50, 0.75, 1.00)
run_cv <- function(alpha_val) {
set.seed(42)
cvfit <- cv.glmnet(
x = X, y = y,
family = "multinomial",
type.measure = "class",
nfolds = NFOLDS_CV,
alpha = alpha_val,
standardize = TRUE
)
lam <- cvfit$lambda.min
# cvm is mean loss (misclassification rate when type.measure="class")
acc <- 1 - cvfit$cvm[which(cvfit$lambda == lam)]
list(cvfit = cvfit, lambda = lam, acc = as.numeric(acc), alpha = alpha_val)
}
results <- lapply(alphas, run_cv)
best <- results[[which.max(vapply(results, function(r) r$acc, numeric(1)))]]
cat(sprintf(
"\nSelected alpha = %.2f | %d-fold CV Accuracy (lambda.min) ≈ %.4f\n",
best$alpha, NFOLDS_CV, best$acc
))
# ------------ Predict & write ------------
pred_test <- predict(best$cvfit, newx = Xt, s = best$lambda, type = "class")
pred_test <- factor(pred_test, levels = levels(y))
submission <- if (!"id" %in% names(test)) {
data.frame(id = seq_len(nrow(test)), Deal = pred_test)
} else {
data.frame(id = test$id, Deal = pred_test)
}
write.csv(submission, "submission.csv", row.names = FALSE)
cat("\nWrote submission.csv (id, Deal)\n")
# ------------ Quick sanity ------------
cat("\nTrain class distribution:\n")
print(prop.table(table(y)))
cat("\nTest prediction distribution:\n")
print(prop.table(table(submission$Deal)))
